{
  "approved_at_utc": null,
  "subreddit": "ripred",
  "selftext": "# Library Architecture\n\nWe design a clean separation between the **game-specific logic** and the **minimax engine**. The library provides a generic minimax/alpha-beta solver, while the user supplies game rules via an interface. This makes the library extensible to different turn-based, deterministic 2-player games (tic-tac-toe, checkers, chess, Connect-4, etc. all share the same minimax structure ([GitHub - JoeStrout/miniscript-alphabeta: standard AI algorithm (Minimax with alpha-beta pruning) for 2-player deterministic games, in MiniScript](https://github.com/JoeStrout/miniscript-alphabeta#:~:text=When%20can%20I%20use%20it%3F)) .\n\n# 2.1 Core Classes and Interfaces\n\n**Game Interface:** We define an abstract interface (in C++ this can be a class with virtual methods) that the user implements for their specific game. This interface includes:\n\n* `evaluateBoard()` \u2013 Return an integer score evaluating the current board state. By convention, a positive score means the state is favorable to the \u201cmaximizing\u201d player, and a negative score favors the opponent ([GitHub - JoeStrout/miniscript-alphabeta: standard AI algorithm (Minimax with alpha-beta pruning) for 2-player deterministic games, in MiniScript](https://github.com/JoeStrout/miniscript-alphabeta#:~:text=5.%20Finally%2C%20implement%20the%20,the%20player%20is%20to%20winning)) Typically, you\u2019d assign +\u221e for a win for the maximizing player, -\u221e for a loss, or use large finite values (e.g. +1000/-1000) to represent win/loss in practice.\n* `generateMoves(Move *moveList)` \u2013 Populate an array with all possible moves from the current state, and return the count of moves. This encapsulates game-specific move generation (all legal moves for the current player). The library will allocate a fixed-size `moveList` array internally (large enough for the worst-case number of moves) to pass to this function.\n* `applyMove(const Move &amp;m)` \u2013 Apply move `m` to the current game state. This should modify the board and typically update whose turn it is. It should **not** allocate memory. Ideally, this is done in-place.\n* `undoMove(const Move &amp;m)` \u2013 Revert move `m`, restoring the previous state. This pairs with `applyMove` to allow backtracking after exploring a move. (If maintaining an explicit move history or using copy-restore, an `undo` function is needed. Alternatively, the library could copy the state for each move instead of modifying in place, but that uses more RAM. Using apply/undo is more memory-efficient, requiring only storing what\u2019s needed to revert the move.)\n* `isGameOver()` \u2013 Return true if the current position is a terminal state (win or draw). This is used to stop the recursion when a game-ending state is reached.\n* `currentPlayer()` \u2013 Indicate whose turn it is (could be an enum or bool). This helps the algorithm determine if we are in a maximizing or minimizing turn. For example, you might use `1` for the maximizing player and `-1` for the minimizing player, or simply use this to check against a known \u201cAI player\u201d ID.\n\nAll these methods are *game-specific*: the library calls these via the interface without knowing the details of the game. For example, in tic-tac-toe, `generateMoves` would find empty cells; in checkers, it would find all legal piece moves (including jumps). By using an interface, we ensure the **minimax core is generic** \u2013 it asks the game object \u201cwhat moves are possible?\u201d and \u201chow good is this state?\u201d, etc., without hard-coding any game rules.\n\n**Minimax Solver Class:** The library\u2019s main class (say `MiniMaxSolver` or similar) contains the implementation of the minimax algorithm with alpha-beta pruning. Key components of this class:\n\n* A reference or pointer to the user\u2019s game state object (implementing the interface). This is how the solver calls the game-specific methods.\n* Configuration such as maximum search depth, and possibly an indicator of which player is the maximizing side (if not inherent in the game state).\n* The minimax search function itself (often implemented recursively), which will use `generateMoves`, `applyMove`, etc., to explore game states.\n* Storage for alpha-beta parameters and best-move tracking. For example, the solver can keep a variable for the **best move found** at the top level, updated during search.\n* (Optional) Buffers for move generation: e.g., an internal static array `Move moveBuffer[MAX_DEPTH][MAX_MOVES]` to store moves at each depth. This avoids allocating new arrays each time. Alternatively, one can allocate a single `Move moves[MAX_MOVES]` array and reuse it at each node (pruning reduces the need for separate buffers per depth). We will ensure `MAX_MOVES` is sufficient for the largest possible branching factor among supported games (for tic-tac-toe, 9; for checkers maybe \\~12; for chess, perhaps up to 30-40 average, though theoretically could be 218 in rare cases). Choosing a safe upper bound like 64 or 128 moves is reasonable, which at a few bytes per move is under 256 bytes of RAM.\n\n**Board and Move Representation:** We keep these representations flexible:\n\n* The **board** can be represented in whatever form the user likes (inside their game class) \u2013 typically a small array or matrix. We encourage using **compact types** (e.g., `uint8_t` for cells or piece counts) and even bitfields/bitboards if appropriate, to save space. For example, a tic-tac-toe board could be stored in just 3 bytes by packing 2-bit codes for each cell ([Tic-Tac-Toe on Arduino (MiniMax)](http://pcarduino.blogspot.com/2014/10/tic-tac-toe-on-arduino-minimax.html#:~:text=struct%20node_t%20%7B%20char%20,3%5D%3B%20char%20children_cnt%3B)) though using a 9-byte `char[9]` array is also fine and more straightforward. The library doesn\u2019t directly access the board; it\u2019s manipulated through the game\u2019s methods.\n* The **Move** struct should be minimal, capturing only essential information for a move. For a simple game, a move might be just an index or coordinate. For a board game, a move might consist of a \u201cfrom\u201d and \u201cto\u201d position (and maybe an extra flag for special moves like promotions). We design `Move` as a small struct (likely 2\u20134 bytes). For example:Tic-tac-toe could use `to` as the cell index and ignore `from`. Checkers could use `from` and `to` (0\u201331 indices for board positions) and perhaps a flag if a piece was crowned. By keeping this struct tiny (and using `uint8_t` or bitfields), we ensure move lists use minimal RAM. Each move\u2019s data will reside either on the stack (during generation) or in our static buffers. No dynamic allocation for moves will occur.struct Move { uint8\\_t from; uint8\\_t to; /\\* maybe uint8\\_t promotion; \\*/ };  \n\n# 2.2 Minimax with Alpha-Beta: Algorithm Design\n\nWe implement the **minimax algorithm** with alpha-beta pruning in a depth-first recursive manner. This approach explores game states one branch at a time, which is very memory-efficient \u2013 we only store a chain of states from root to leaf, rather than the whole tree ([Tic-Tac-Toe on Arduino (MiniMax)](http://pcarduino.blogspot.com/2014/10/tic-tac-toe-on-arduino-minimax.html#:~:text=It%27s%20still%20possible%20to%20build,a%20lot%20more%20than%20enough)) Here\u2019s how the algorithm works in our context:\n\n* **Recursive Function:** We define a function (let\u2019s call it `minimaxSearch`) that takes parameters like `depth` (remaining depth to search), `alpha` and `beta` (the current alpha-beta bounds), and perhaps an indicator of whether we are maximizing or minimizing. This function will:Check if the game is over or `depth == 0` (reached maximum depth). If so, call `evaluateBoard()` and return the evaluation. This is the terminal condition of recursion.Otherwise, generate all possible moves by calling `generateMoves()`. Iterate through each move:Return the best score found for this node.Apply the move (`applyMove`) to transition to the new state.Recursively call `minimaxSearch(depth-1, alpha, beta, otherPlayer)` to evaluate the resulting position. (The `otherPlayer` flag flips the role: if we were maximizing, now we minimize, and vice versa.)Undo the move (`undoMove`) to restore the state for the next move in the loop.Use the returned score to update our best score:If we are the maximizing player, we look for the **maximum** score. If the new score is higher than the best so far, update the best. Also update `alpha = max(alpha, score)`. If at any point `alpha &gt;= beta`, we can **prune** (break out of the loop) because the minimizing player would avoid this branch ([Alpha Beta Pruning in Artificial Intelligence](https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:~:text=,won%E2%80%99t%20impact%20the%20final%20decision))If we are the minimizing player, we look for the **minimum** score. Update best (min) and `beta = min(beta, score)`. If `beta &lt;= alpha`, prune (the maximizer would never let this scenario happen).\n* **Alpha-Beta Pruning:** By carrying the `alpha` (best value for max so far) and `beta` (best for min so far) through the recursion, we drastically cut off branches that cannot produce a better outcome than previously examined moves ([Alpha Beta Pruning in Artificial Intelligence](https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:~:text=,won%E2%80%99t%20impact%20the%20final%20decision)) For instance, if we find a move that results in a score X for the maximizing player, any alternative move the opponent might make that yields a result worse than X for the opponent (i.e., better for the maximizing player) can be skipped \u2013 the opponent will choose the move that leads to X or better for themselves. In practice, alpha-beta pruning can reduce the effective branching factor significantly, allowing deeper searches on the same hardware ([Alpha Beta Pruning in Artificial Intelligence](https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:~:text=Impact%20of%20Alpha,Search%20Depth))\n* **Negamax Implementation:** We can simplify the minimax logic using the **negamax** pattern (since two-player zero-sum games are symmetric). In negamax, we use one recursive function for both players, and encode the player perspective by flipping the sign of scores. For example, one can implement:In this scheme, `evaluateBoard()` should return a score from the perspective of the **current player to move**. The recursive call negates the score (`-minimaxSearch`) and swaps alpha/beta signs, which effectively handles the min/max inversion ([Tic-Tac-Toe on Arduino (MiniMax)](http://pcarduino.blogspot.com/2014/10/tic-tac-toe-on-arduino-minimax.html#:~:text=int%20negaMax,return%20max)) Negamax reduces code duplication (we don\u2019t need separate min and max logic), but it requires careful design of the evaluation function. Alternatively, one can write it with explicit maximize/minimize branches \u2013 conceptually the result is the same. For clarity in this report, we might present the algorithm in the more traditional min/max form with if/else for maximizing vs minimizing [player.int](http://player.int) minimaxSearch(int depth, int alpha, int beta) {     if (game.isGameOver() || depth == 0) {         return game.evaluateBoard();     }     int maxValue = -INFINITY;     Move moves\\[MAX\\_MOVES\\];     uint8\\_t moveCount = game.generateMoves(moves);     for (uint8\\_t i = 0; i &lt; moveCount; ++i) {         game.applyMove(moves\\[i\\]);         // Recurse for the opponent with inverted alpha/beta         int score = -minimaxSearch(depth - 1, -beta, -alpha);         game.undoMove(moves\\[i\\]);         if (score &gt; maxValue) {             maxValue = score;         }         if (score &gt; alpha) {             alpha = score;         }         if (alpha &gt;= beta) {             break;  // alpha-beta cutoff         }     }     return maxValue; }  \n\n**Pseudocode (Max/Min version)** for clarity, with alpha-beta:\n\n    function minimax(node, depth, alpha, beta, maximizingPlayer):     if depth == 0 or node.isGameOver():         return node.evaluateBoard()    // static evaluation of terminal or depth limit          if maximizingPlayer:         int maxEval = -INF;         Move moves[MAX_MOVES];         int count = node.generateMoves(moves);         for (int i = 0; i &lt; count; ++i):             node.applyMove(moves[i]);             int eval = minimax(node, depth-1, alpha, beta, false);             node.undoMove(moves[i]);             if (eval &gt; maxEval):                 maxEval = eval;             alpha = max(alpha, eval);             if (alpha &gt;= beta):                 break;      // beta cut-off         return maxEval;     else:         int minEval = +INF;         Move moves[MAX_MOVES];         int count = node.generateMoves(moves);         for (int i = 0; i &lt; count; ++i):             node.applyMove(moves[i]);             int eval = minimax(node, depth-1, alpha, beta, true);             node.undoMove(moves[i]);             if (eval &lt; minEval):                 minEval = eval;             beta = min(beta, eval);             if (alpha &gt;= beta):                 break;      // alpha cut-off         return minEval; \n\nThis algorithm will return the best achievable score from the current position (assuming optimal play by both sides) up to the given depth. The initial call (from the library user) would be something like:\n\n    bestScore = minimax(rootNode, maxDepth, -INF, +INF, /*maximizingPlayer=*/true); \n\nThe library will track which move led to `bestScore` at the root and return that move as the AI\u2019s chosen move.\n\n**Efficiency considerations:** With alpha-beta and decent move ordering, the algorithm will prune a large portion of the tree. In an optimal scenario (best moves always encountered first), alpha-beta can achieve roughly $O(b^({d/2})$) complexity instead of $O(b^(d)$) for minimax, where $b$ is branching factor and $d$ depth ([Alpha Beta Pruning in Artificial Intelligence](https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:~:text=Impact%20of%20Alpha,Search%20Depth)) Even if not optimal, it\u2019s a substantial improvement. For example, a full minimax on tic-tac-toe (b \\~ 9, d up to 9) examines 9! = 362k nodes; alpha-beta might cut that down to tens of thousands. On an ATmega328P, this is easily handled. For more complex games like chess with huge $b$, alpha-beta plus heuristics is the only way to search any meaningful depth.\n\n**Move Ordering:** We will integrate simple heuristics to sort moves **before** recursion:\n\n* If the user\u2019s game logic can identify move priorities (e.g., a winning move, or captures), they can either generate those first or we can provide a hook to rank moves. For simplicity, the user could partially sort moves in `generateMoves()` itself (e.g., by adding likely good moves to the list first). For instance, one could generate all moves and then swap the best-looking move to the front.\n* Alternatively, the library can do a one-step evaluation: apply each move, call `evaluateBoard()`, store the scores, undo the move, then sort the move list by score (descending for maximizer, ascending for minimizer) before the main loop. This is essentially a **shallow search move ordering**, which is known to improve pruning effectiveness ([Alpha Beta Pruning in Artificial Intelligence](https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:~:text=idea%20is%20to%20perform%20a,pruning%20of%20less%20promising%20moves)) Because our moves per position are usually limited (especially in small board games), the overhead of this sorting is small compared to the deeper search savings.\n* We will keep the implementation of move ordering straightforward to preserve code size. Even without complex schemes like \u201ckiller move\u201d or \u201chistory heuristic\u201d from advanced chess engines, basic ordering yields a noticeable speedup ([Alpha Beta Pruning in Artificial Intelligence](https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:~:text=,the%20algorithm%20even%20more%20efficient))\n\n**Depth Limitation and Quiescence:** We may allow the user to specify `maxDepth` for search. In games with potential for long forced sequences (like many jumps in checkers or multiple captures in chess), a fixed depth might cut off in the middle of a volatile situation. A full solution would use **quiescence search** (continuing the search until the position is quiet, i.e., no immediate capture threats). MicroChess, for example, includes a quiescent search extension ([GitHub - ripred/MicroChess: A full featured chess engine designed to fit in an embedded environment, using less than 2K of RAM!](https://github.com/ripred/MicroChess#:~:text=MicroChess%20is%20an%20embedded%20chess,capture%2C%20castling%2C%20and%20quiescent%20searches)) However, to keep our library simpler and within time limits, we won\u2019t implement quiescence by default (it can be added for games that need it). Instead, users can slightly increase depth or incorporate capture sequences in move generation logic to mitigate the horizon effect.\n\n**Memory Use During Search:** Thanks to recursion and in-place move application, memory usage is modest. We require roughly:\n\n* *Stack frame per depth:* a few local variables (score, loop counters, etc.) plus whatever the game\u2019s `applyMove` and `evaluateBoard` use. Empirically, a well-optimized engine used \\~142 bytes per ply in a chess scenario ([Writing an Embedded Chess Engine - Part 5 - Showcase - Arduino Forum](https://forum.arduino.cc/t/writing-an-embedded-chess-engine-part-5/1130748#:~:text=Extensive%20effort%20has%20gone%20into,running%20under%202K%20of%20RAM)) Simpler games will use far less. An 8-deep recursion for tic-tac-toe might consume well under 128 bytes total ([Tic-Tac-Toe on Arduino (MiniMax)](http://pcarduino.blogspot.com/2014/10/tic-tac-toe-on-arduino-minimax.html#:~:text=It%27s%20still%20possible%20to%20build,a%20lot%20more%20than%20enough))\n* *Move list storage:* one array of `Move` of length MAX\\_MOVES, which can be on the stack or static. If static global, it uses fixed SRAM but doesn\u2019t grow with depth. If allocated on stack in each call, it multiplies by depth (which might be okay for shallow depths, but risky for deeper ones). A compromise is to use a **global 2D buffer** `Move movesByDepth[MAX_DEPTH][MAX_MOVES]` and pass a pointer to the appropriate sub-array for each recursion level. This way, we don\u2019t allocate new memory per call (it\u2019s all in global), and we avoid interference between levels. The cost is MAX\\_DEPTH \\* MAX\\_MOVES \\* sizeof(Move) bytes of SRAM. For example, if MAX\\_DEPTH=10 and MAX\\_MOVES=64 and Move=2 bytes, that\u2019s 10\\_64\\_2 = 1280 bytes, which is a large chunk of 2KB. We can tune these numbers per game or use smaller buffers if the game inherently has lower branching. Another approach is to generate moves and process them immediately, one by one, without storing the whole list \u2013 but that complicates backtracking. **We will assume a reasonable upper bound and document that if a user\u2019s game exceeds it, they should adjust MAX\\_MOVES or search depth accordingly.**\n* *Board state storage:* If using `applyMove`/`undoMove`, we only maintain one copy of the board (inside the game object) plus any small info needed to undo (for instance, remembering a captured piece). This is extremely memory-efficient. If we opted to copy the board for each move instead, we\u2019d need to allocate a new board state at each node. That approach was considered in the tic-tac-toe example and quickly found impractical: even a 3-level tree consumed \\~3024 bytes when copying board nodes ([Tic-Tac-Toe on Arduino (MiniMax)](http://pcarduino.blogspot.com/2014/10/tic-tac-toe-on-arduino-minimax.html#:~:text=Which%20is%206%20bytes%20per,children%2C%20which%20would%20give%20roughly)) Our in-place approach avoids that explosion. It does require that `undoMove` correctly restores all aspects of state (board cells, whose turn, etc.), which the user must implement carefully. When done right, only a few bytes (to store what was changed) are needed per move.\n\n# 2.3 Library Integration and Usage\n\nThe library will be packaged as a standard Arduino library with a header (`MinimaxAI.h`) and source (`MinimaxAI.cpp`), and one or more example sketches in an `examples/` folder. It will be Arduino IDE compatible (the classes use `Arduino.h` if needed, and avoid unsupported constructs).\n\n**Using the Library:**\n\n1. **Include and Initialize:** In the user\u2019s sketch (`.ino`), they include the library header and create an instance of their game state class (which implements the interface). They also create the Minimax solver instance, passing a reference to the game and any config (like max search depth).Depending on implementation, `MinimaxAI` might be a class template that takes the game class type (allowing inlining and compile-time binding of game methods, which could save the overhead of virtual calls). Or it could use a base class pointer (`GameInterface *game`) internally (simpler to understand, but each interface call is a virtual function call). Given the very low performance overhead in small games and simplicity for the user, we might go with an abstract base class `GameInterface` that `TicTacToeGame` inherits. For maximum speed in critical loops, a template can inline game logic, but it increases code size per game.#include &lt;MinimaxAI.h&gt; TicTacToeGame game;                // user-defined game state MinimaxAI&lt;TicTacToeGame&gt; ai(game, /\\*depth=\\*/9);  // template or concrete class instance  \n2. **Implement Game Logic:** The user must implement the required methods in their game class. Here\u2019s a snippet example for Tic-Tac-Toe:In the above implementation:enum Player { HUMAN = 0, AI = 1 };  // define players class TicTacToeGame : public GameInterface { public:     uint8\\_t board\\[9\\];    // 3x3 board stored in 1D (0 = empty, 1 = X, 2 = O)     Player current;      // whose turn it is     TicTacToeGame() {         memset(board, 0, 9);         current = AI;    // AI starts (for example)     }     int evaluateBoard() override {         // Evaluate from AI's perspective (AI = 'X' say = 1, Human = 'O' = 2)         // Return +10 for AI win, -10 for Human win, 0 for draw/ongoing.         if (isWinner(1)) return +10;         if (isWinner(2)) return -10;         // If game not over, return 0 (or small heuristic: e.g., +1 for two-in-a-row)         return 0;     }     uint8\\_t generateMoves(Move \\*moveList) override {         uint8\\_t count = 0;         for (uint8\\_t i = 0; i &lt; 9; ++i) {             if (board\\[i\\] == 0) {                 moveList\\[count++\\] = Move{i, i}; // use 'to' as i; from unused             }         }         return count;     }     void applyMove(const Move &amp;m) override {         uint8\\_t cell = [m.to](http://m.to);         board\\[cell\\] = (current == AI ? 1 : 2);         // switch player turn         current = (current == AI ? HUMAN : AI);     }     void undoMove(const Move &amp;m) override {         uint8\\_t cell = [m.to](http://m.to);         // remove the mark and switch back turn         board\\[cell\\] = 0;         current = (current == AI ? HUMAN : AI);     }     bool isGameOver() override {         return isWinner(1) || isWinner(2) || isBoardFull();     }     int currentPlayer() override {         return (current == AI ? 1 : -1);          // 1 for AI (maximizing), -1 for human (minimizing)     } private:     bool isWinner(uint8\\_t playerVal) {         // check 3 rows, 3 cols, 2 diagonals for all == playerVal         // ... (omitted for brevity)     }     bool isBoardFull() {         for (uint8\\_t i = 0; i &lt; 9; ++i) if (board\\[i\\] == 0) return false;         return true;     } };  We encode X as `1` and O as `2` on the board. The `evaluateBoard` knows that AI is X (1) and Human is O (2), and returns positive scores for AI-winning states.`generateMoves` lists all empty cells as possible moves.`applyMove` and `undoMove` simply place or remove a mark and toggle the `current` player. (Toggling is done by checking the `current` and swapping \u2013 since we only have two players, this is straightforward.)`currentPlayer()` returns an int indicating if the current turn is the maximizing player or not. Here we chose AI as maximizing (return 1) and human as minimizing (return -1). The minimax solver could also determine this by comparing `current` to a stored \u201cmaximizing player\u201d identity.\n3. **Running the AI:** To get the AI\u2019s move, the user calls a method from `MinimaxAI`, for example:Internally, `findBestMove()` will call the `minimaxSearch` (with appropriate initial parameters: full depth, alpha=-inf, beta=+inf, and maximizing = true/false depending on `game.currentPlayer()`). It will iterate over moves at the root to find the one that leads to the optimal score. The result is returned as a `Move` struct. The user can then apply it to the game:(Alternatively, the `findBestMove()` could optionally apply the move for the user, but returning it gives more flexibility.)Move best = ai.findBestMove();  game.applyMove(best);  \n4. **Serial Interface for Moves:** Our example sketches will demonstrate using Serial to interact:The Arduino could prompt for input like a cell number or move notation.The user enters a move (e.g., \u201c5\u201d for center cell in tic-tac-toe, or \u201c12-16\u201d in checkers notation). The sketch code will parse this and call `game.applyMove` for the human\u2019s move.Then the AI move is computed and printed out, e.g., \u201cAI moves to 7\u201d.In a loop, this continues until `game.isGameOver()` becomes true, at which point the result (win/draw) is announced.We ensure the example is easy to follow. For instance, we might represent tic-tac-toe board positions 1\u20139 and have the user enter a number. The code maps that to our 0\u20138 index and makes the move.\n5. **Installing and Extending:** The library will come with documentation (in the report and comments) describing how to define a new game class with the required methods. To use the library for another game, the user basically re-implements a class similar to `TicTacToeGame` (for, say, `CheckersGame` or `Connect4Game`), writing the logic for move generation, evaluation, etc. They can then use the same `MinimaxAI` class to get AI moves for that game. Because everything is static and no dynamic memory is used, even more complex games should fit. For example, a Checkers game might use an 8x8 board (64 bytes) and have a branching factor averaging &lt; 12 moves. If we limit depth to perhaps 6 plies, the search might examine thousands of positions \u2013 which is slow but feasible (the AI may take a few seconds on Arduino for depth 6). Simpler games like Connect-4 (7x6 board) or Othello (8x8) would similarly work with adjusted depth. Chess, being much more complex, would need additional optimizations (as done in MicroChess, which uses bitboards and specialized move generation to run under 2KB ([GitHub - ripred/MicroChess: A full featured chess engine designed to fit in an embedded environment, using less than 2K of RAM!](https://github.com/ripred/MicroChess#:~:text=The%20design%20includes%20use%20of,games%20including%20chess%20and%20checkers)) , but our framework could theoretically support it at shallow depths.\n\n**Memory Footprint of the Library:** The code itself (minimax implementation) will reside in flash. We strive to keep it concise. Code size is likely on the order of a few kilobytes \u2013 well within 32KB flash. The global/static memory usage consists of the move buffer and any other static structures (which we aim to keep under a few hundred bytes). The game state itself is also often small (tic-tac-toe game state here is 9 bytes + a couple of variables; checkers might be \\~32 bytes for pieces plus some bookkeeping). As a concrete data point, **MicroChess** (a full chess engine) uses \\~810 bytes static and leaves \\~1238 bytes for runtime stack ([Writing an Embedded Chess Engine - Part 5 - Showcase - Arduino Forum](https://forum.arduino.cc/t/writing-an-embedded-chess-engine-part-5/1130748#:~:text=Extensive%20effort%20has%20gone%20into,running%20under%202K%20of%20RAM)) Our tic-tac-toe example will use far less. This shows there is ample headroom if carefully managed. By following similar strategies (bit-packing data, avoiding large arrays), one can keep within the Uno\u2019s limits for many games.",
  "author_fullname": "t2_adfkq",
  "saved": false,
  "mod_reason_title": null,
  "gilded": 0,
  "clicked": false,
  "title": "The Amazing Minimax Algorithm (and Why You Should Use It in Your Games!) Pt 2",
  "link_flair_richtext": [
    {
      "e": "text",
      "t": "Algorithms"
    }
  ],
  "subreddit_name_prefixed": "r/ripred",
  "hidden": false,
  "pwls": null,
  "link_flair_css_class": "",
  "downs": 0,
  "thumbnail_height": null,
  "top_awarded_type": null,
  "hide_score": false,
  "name": "t3_1ihldqx",
  "quarantine": false,
  "link_flair_text_color": "light",
  "upvote_ratio": 1.0,
  "author_flair_background_color": "transparent",
  "subreddit_type": "public",
  "ups": 1,
  "total_awards_received": 0,
  "media_embed": {},
  "thumbnail_width": null,
  "author_flair_template_id": "9fa2ceaa-053c-11ed-bb97-124dff5ea4b4",
  "is_original_content": false,
  "user_reports": [],
  "secure_media": null,
  "is_reddit_media_domain": false,
  "is_meta": false,
  "category": null,
  "secure_media_embed": {},
  "link_flair_text": "Algorithms",
  "can_mod_post": false,
  "score": 1,
  "approved_by": null,
  "is_created_from_ads_ui": false,
  "author_premium": false,
  "thumbnail": "self",
  "edited": false,
  "author_flair_css_class": null,
  "author_flair_richtext": [
    {
      "a": ":snoo_facepalm:",
      "e": "emoji",
      "u": "https://emoji.redditmedia.com/wzxf63qpaezz_t5_3nqvj/snoo_facepalm"
    }
  ],
  "gildings": {},
  "content_categories": null,
  "is_self": true,
  "mod_note": null,
  "created": 1738685857.0,
  "link_flair_type": "richtext",
  "wls": null,
  "removed_by_category": null,
  "banned_by": null,
  "author_flair_type": "richtext",
  "domain": "self.ripred",
  "allow_live_comments": false,
  "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Library Architecture&lt;/h1&gt;\n\n&lt;p&gt;We design a clean separation between the &lt;strong&gt;game-specific logic&lt;/strong&gt; and the &lt;strong&gt;minimax engine&lt;/strong&gt;. The library provides a generic minimax/alpha-beta solver, while the user supplies game rules via an interface. This makes the library extensible to different turn-based, deterministic 2-player games (tic-tac-toe, checkers, chess, Connect-4, etc. all share the same minimax structure (&lt;a href=\"https://github.com/JoeStrout/miniscript-alphabeta#:%7E:text=When%20can%20I%20use%20it%3F\"&gt;GitHub - JoeStrout/miniscript-alphabeta: standard AI algorithm (Minimax with alpha-beta pruning) for 2-player deterministic games, in MiniScript&lt;/a&gt;) .&lt;/p&gt;\n\n&lt;h1&gt;2.1 Core Classes and Interfaces&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Game Interface:&lt;/strong&gt; We define an abstract interface (in C++ this can be a class with virtual methods) that the user implements for their specific game. This interface includes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;evaluateBoard()&lt;/code&gt; \u2013 Return an integer score evaluating the current board state. By convention, a positive score means the state is favorable to the \u201cmaximizing\u201d player, and a negative score favors the opponent (&lt;a href=\"https://github.com/JoeStrout/miniscript-alphabeta#:%7E:text=5.%20Finally%2C%20implement%20the%20,the%20player%20is%20to%20winning\"&gt;GitHub - JoeStrout/miniscript-alphabeta: standard AI algorithm (Minimax with alpha-beta pruning) for 2-player deterministic games, in MiniScript&lt;/a&gt;) Typically, you\u2019d assign +\u221e for a win for the maximizing player, -\u221e for a loss, or use large finite values (e.g. +1000/-1000) to represent win/loss in practice.&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;generateMoves(Move *moveList)&lt;/code&gt; \u2013 Populate an array with all possible moves from the current state, and return the count of moves. This encapsulates game-specific move generation (all legal moves for the current player). The library will allocate a fixed-size &lt;code&gt;moveList&lt;/code&gt; array internally (large enough for the worst-case number of moves) to pass to this function.&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;applyMove(const Move &amp;amp;m)&lt;/code&gt; \u2013 Apply move &lt;code&gt;m&lt;/code&gt; to the current game state. This should modify the board and typically update whose turn it is. It should &lt;strong&gt;not&lt;/strong&gt; allocate memory. Ideally, this is done in-place.&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;undoMove(const Move &amp;amp;m)&lt;/code&gt; \u2013 Revert move &lt;code&gt;m&lt;/code&gt;, restoring the previous state. This pairs with &lt;code&gt;applyMove&lt;/code&gt; to allow backtracking after exploring a move. (If maintaining an explicit move history or using copy-restore, an &lt;code&gt;undo&lt;/code&gt; function is needed. Alternatively, the library could copy the state for each move instead of modifying in place, but that uses more RAM. Using apply/undo is more memory-efficient, requiring only storing what\u2019s needed to revert the move.)&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;isGameOver()&lt;/code&gt; \u2013 Return true if the current position is a terminal state (win or draw). This is used to stop the recursion when a game-ending state is reached.&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;currentPlayer()&lt;/code&gt; \u2013 Indicate whose turn it is (could be an enum or bool). This helps the algorithm determine if we are in a maximizing or minimizing turn. For example, you might use &lt;code&gt;1&lt;/code&gt; for the maximizing player and &lt;code&gt;-1&lt;/code&gt; for the minimizing player, or simply use this to check against a known \u201cAI player\u201d ID.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;All these methods are &lt;em&gt;game-specific&lt;/em&gt;: the library calls these via the interface without knowing the details of the game. For example, in tic-tac-toe, &lt;code&gt;generateMoves&lt;/code&gt; would find empty cells; in checkers, it would find all legal piece moves (including jumps). By using an interface, we ensure the &lt;strong&gt;minimax core is generic&lt;/strong&gt; \u2013 it asks the game object \u201cwhat moves are possible?\u201d and \u201chow good is this state?\u201d, etc., without hard-coding any game rules.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Minimax Solver Class:&lt;/strong&gt; The library\u2019s main class (say &lt;code&gt;MiniMaxSolver&lt;/code&gt; or similar) contains the implementation of the minimax algorithm with alpha-beta pruning. Key components of this class:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A reference or pointer to the user\u2019s game state object (implementing the interface). This is how the solver calls the game-specific methods.&lt;/li&gt;\n&lt;li&gt;Configuration such as maximum search depth, and possibly an indicator of which player is the maximizing side (if not inherent in the game state).&lt;/li&gt;\n&lt;li&gt;The minimax search function itself (often implemented recursively), which will use &lt;code&gt;generateMoves&lt;/code&gt;, &lt;code&gt;applyMove&lt;/code&gt;, etc., to explore game states.&lt;/li&gt;\n&lt;li&gt;Storage for alpha-beta parameters and best-move tracking. For example, the solver can keep a variable for the &lt;strong&gt;best move found&lt;/strong&gt; at the top level, updated during search.&lt;/li&gt;\n&lt;li&gt;(Optional) Buffers for move generation: e.g., an internal static array &lt;code&gt;Move moveBuffer[MAX_DEPTH][MAX_MOVES]&lt;/code&gt; to store moves at each depth. This avoids allocating new arrays each time. Alternatively, one can allocate a single &lt;code&gt;Move moves[MAX_MOVES]&lt;/code&gt; array and reuse it at each node (pruning reduces the need for separate buffers per depth). We will ensure &lt;code&gt;MAX_MOVES&lt;/code&gt; is sufficient for the largest possible branching factor among supported games (for tic-tac-toe, 9; for checkers maybe ~12; for chess, perhaps up to 30-40 average, though theoretically could be 218 in rare cases). Choosing a safe upper bound like 64 or 128 moves is reasonable, which at a few bytes per move is under 256 bytes of RAM.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Board and Move Representation:&lt;/strong&gt; We keep these representations flexible:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The &lt;strong&gt;board&lt;/strong&gt; can be represented in whatever form the user likes (inside their game class) \u2013 typically a small array or matrix. We encourage using &lt;strong&gt;compact types&lt;/strong&gt; (e.g., &lt;code&gt;uint8_t&lt;/code&gt; for cells or piece counts) and even bitfields/bitboards if appropriate, to save space. For example, a tic-tac-toe board could be stored in just 3 bytes by packing 2-bit codes for each cell (&lt;a href=\"http://pcarduino.blogspot.com/2014/10/tic-tac-toe-on-arduino-minimax.html#:%7E:text=struct%20node_t%20%7B%20char%20,3%5D%3B%20char%20children_cnt%3B\"&gt;Tic-Tac-Toe on Arduino (MiniMax)&lt;/a&gt;) though using a 9-byte &lt;code&gt;char[9]&lt;/code&gt; array is also fine and more straightforward. The library doesn\u2019t directly access the board; it\u2019s manipulated through the game\u2019s methods.&lt;/li&gt;\n&lt;li&gt;The &lt;strong&gt;Move&lt;/strong&gt; struct should be minimal, capturing only essential information for a move. For a simple game, a move might be just an index or coordinate. For a board game, a move might consist of a \u201cfrom\u201d and \u201cto\u201d position (and maybe an extra flag for special moves like promotions). We design &lt;code&gt;Move&lt;/code&gt; as a small struct (likely 2\u20134 bytes). For example:Tic-tac-toe could use &lt;code&gt;to&lt;/code&gt; as the cell index and ignore &lt;code&gt;from&lt;/code&gt;. Checkers could use &lt;code&gt;from&lt;/code&gt; and &lt;code&gt;to&lt;/code&gt; (0\u201331 indices for board positions) and perhaps a flag if a piece was crowned. By keeping this struct tiny (and using &lt;code&gt;uint8_t&lt;/code&gt; or bitfields), we ensure move lists use minimal RAM. Each move\u2019s data will reside either on the stack (during generation) or in our static buffers. No dynamic allocation for moves will occur.struct Move { uint8_t from; uint8_t to; /* maybe uint8_t promotion; */ };&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;2.2 Minimax with Alpha-Beta: Algorithm Design&lt;/h1&gt;\n\n&lt;p&gt;We implement the &lt;strong&gt;minimax algorithm&lt;/strong&gt; with alpha-beta pruning in a depth-first recursive manner. This approach explores game states one branch at a time, which is very memory-efficient \u2013 we only store a chain of states from root to leaf, rather than the whole tree (&lt;a href=\"http://pcarduino.blogspot.com/2014/10/tic-tac-toe-on-arduino-minimax.html#:%7E:text=It%27s%20still%20possible%20to%20build,a%20lot%20more%20than%20enough\"&gt;Tic-Tac-Toe on Arduino (MiniMax)&lt;/a&gt;) Here\u2019s how the algorithm works in our context:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Recursive Function:&lt;/strong&gt; We define a function (let\u2019s call it &lt;code&gt;minimaxSearch&lt;/code&gt;) that takes parameters like &lt;code&gt;depth&lt;/code&gt; (remaining depth to search), &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;beta&lt;/code&gt; (the current alpha-beta bounds), and perhaps an indicator of whether we are maximizing or minimizing. This function will:Check if the game is over or &lt;code&gt;depth == 0&lt;/code&gt; (reached maximum depth). If so, call &lt;code&gt;evaluateBoard()&lt;/code&gt; and return the evaluation. This is the terminal condition of recursion.Otherwise, generate all possible moves by calling &lt;code&gt;generateMoves()&lt;/code&gt;. Iterate through each move:Return the best score found for this node.Apply the move (&lt;code&gt;applyMove&lt;/code&gt;) to transition to the new state.Recursively call &lt;code&gt;minimaxSearch(depth-1, alpha, beta, otherPlayer)&lt;/code&gt; to evaluate the resulting position. (The &lt;code&gt;otherPlayer&lt;/code&gt; flag flips the role: if we were maximizing, now we minimize, and vice versa.)Undo the move (&lt;code&gt;undoMove&lt;/code&gt;) to restore the state for the next move in the loop.Use the returned score to update our best score:If we are the maximizing player, we look for the &lt;strong&gt;maximum&lt;/strong&gt; score. If the new score is higher than the best so far, update the best. Also update &lt;code&gt;alpha = max(alpha, score)&lt;/code&gt;. If at any point &lt;code&gt;alpha &amp;gt;= beta&lt;/code&gt;, we can &lt;strong&gt;prune&lt;/strong&gt; (break out of the loop) because the minimizing player would avoid this branch (&lt;a href=\"https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:%7E:text=,won%E2%80%99t%20impact%20the%20final%20decision\"&gt;Alpha Beta Pruning in Artificial Intelligence&lt;/a&gt;)If we are the minimizing player, we look for the &lt;strong&gt;minimum&lt;/strong&gt; score. Update best (min) and &lt;code&gt;beta = min(beta, score)&lt;/code&gt;. If &lt;code&gt;beta &amp;lt;= alpha&lt;/code&gt;, prune (the maximizer would never let this scenario happen).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Alpha-Beta Pruning:&lt;/strong&gt; By carrying the &lt;code&gt;alpha&lt;/code&gt; (best value for max so far) and &lt;code&gt;beta&lt;/code&gt; (best for min so far) through the recursion, we drastically cut off branches that cannot produce a better outcome than previously examined moves (&lt;a href=\"https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:%7E:text=,won%E2%80%99t%20impact%20the%20final%20decision\"&gt;Alpha Beta Pruning in Artificial Intelligence&lt;/a&gt;) For instance, if we find a move that results in a score X for the maximizing player, any alternative move the opponent might make that yields a result worse than X for the opponent (i.e., better for the maximizing player) can be skipped \u2013 the opponent will choose the move that leads to X or better for themselves. In practice, alpha-beta pruning can reduce the effective branching factor significantly, allowing deeper searches on the same hardware (&lt;a href=\"https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:%7E:text=Impact%20of%20Alpha,Search%20Depth\"&gt;Alpha Beta Pruning in Artificial Intelligence&lt;/a&gt;)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Negamax Implementation:&lt;/strong&gt; We can simplify the minimax logic using the &lt;strong&gt;negamax&lt;/strong&gt; pattern (since two-player zero-sum games are symmetric). In negamax, we use one recursive function for both players, and encode the player perspective by flipping the sign of scores. For example, one can implement:In this scheme, &lt;code&gt;evaluateBoard()&lt;/code&gt; should return a score from the perspective of the &lt;strong&gt;current player to move&lt;/strong&gt;. The recursive call negates the score (&lt;code&gt;-minimaxSearch&lt;/code&gt;) and swaps alpha/beta signs, which effectively handles the min/max inversion (&lt;a href=\"http://pcarduino.blogspot.com/2014/10/tic-tac-toe-on-arduino-minimax.html#:%7E:text=int%20negaMax,return%20max\"&gt;Tic-Tac-Toe on Arduino (MiniMax)&lt;/a&gt;) Negamax reduces code duplication (we don\u2019t need separate min and max logic), but it requires careful design of the evaluation function. Alternatively, one can write it with explicit maximize/minimize branches \u2013 conceptually the result is the same. For clarity in this report, we might present the algorithm in the more traditional min/max form with if/else for maximizing vs minimizing &lt;a href=\"http://player.int\"&gt;player.int&lt;/a&gt; minimaxSearch(int depth, int alpha, int beta) {     if (game.isGameOver() || depth == 0) {         return game.evaluateBoard();     }     int maxValue = -INFINITY;     Move moves[MAX_MOVES];     uint8_t moveCount = game.generateMoves(moves);     for (uint8_t i = 0; i &amp;lt; moveCount; ++i) {         game.applyMove(moves[i]);         // Recurse for the opponent with inverted alpha/beta         int score = -minimaxSearch(depth - 1, -beta, -alpha);         game.undoMove(moves[i]);         if (score &amp;gt; maxValue) {             maxValue = score;         }         if (score &amp;gt; alpha) {             alpha = score;         }         if (alpha &amp;gt;= beta) {             break;  // alpha-beta cutoff         }     }     return maxValue; }&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Pseudocode (Max/Min version)&lt;/strong&gt; for clarity, with alpha-beta:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;function minimax(node, depth, alpha, beta, maximizingPlayer):     if depth == 0 or node.isGameOver():         return node.evaluateBoard()    // static evaluation of terminal or depth limit          if maximizingPlayer:         int maxEval = -INF;         Move moves[MAX_MOVES];         int count = node.generateMoves(moves);         for (int i = 0; i &amp;lt; count; ++i):             node.applyMove(moves[i]);             int eval = minimax(node, depth-1, alpha, beta, false);             node.undoMove(moves[i]);             if (eval &amp;gt; maxEval):                 maxEval = eval;             alpha = max(alpha, eval);             if (alpha &amp;gt;= beta):                 break;      // beta cut-off         return maxEval;     else:         int minEval = +INF;         Move moves[MAX_MOVES];         int count = node.generateMoves(moves);         for (int i = 0; i &amp;lt; count; ++i):             node.applyMove(moves[i]);             int eval = minimax(node, depth-1, alpha, beta, true);             node.undoMove(moves[i]);             if (eval &amp;lt; minEval):                 minEval = eval;             beta = min(beta, eval);             if (alpha &amp;gt;= beta):                 break;      // alpha cut-off         return minEval; \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This algorithm will return the best achievable score from the current position (assuming optimal play by both sides) up to the given depth. The initial call (from the library user) would be something like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;bestScore = minimax(rootNode, maxDepth, -INF, +INF, /*maximizingPlayer=*/true); \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The library will track which move led to &lt;code&gt;bestScore&lt;/code&gt; at the root and return that move as the AI\u2019s chosen move.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Efficiency considerations:&lt;/strong&gt; With alpha-beta and decent move ordering, the algorithm will prune a large portion of the tree. In an optimal scenario (best moves always encountered first), alpha-beta can achieve roughly $O(b&lt;sup&gt;{d/2}&lt;/sup&gt;$) complexity instead of $O(b&lt;sup&gt;d&lt;/sup&gt;$) for minimax, where $b$ is branching factor and $d$ depth (&lt;a href=\"https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:%7E:text=Impact%20of%20Alpha,Search%20Depth\"&gt;Alpha Beta Pruning in Artificial Intelligence&lt;/a&gt;) Even if not optimal, it\u2019s a substantial improvement. For example, a full minimax on tic-tac-toe (b ~ 9, d up to 9) examines 9! = 362k nodes; alpha-beta might cut that down to tens of thousands. On an ATmega328P, this is easily handled. For more complex games like chess with huge $b$, alpha-beta plus heuristics is the only way to search any meaningful depth.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Move Ordering:&lt;/strong&gt; We will integrate simple heuristics to sort moves &lt;strong&gt;before&lt;/strong&gt; recursion:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If the user\u2019s game logic can identify move priorities (e.g., a winning move, or captures), they can either generate those first or we can provide a hook to rank moves. For simplicity, the user could partially sort moves in &lt;code&gt;generateMoves()&lt;/code&gt; itself (e.g., by adding likely good moves to the list first). For instance, one could generate all moves and then swap the best-looking move to the front.&lt;/li&gt;\n&lt;li&gt;Alternatively, the library can do a one-step evaluation: apply each move, call &lt;code&gt;evaluateBoard()&lt;/code&gt;, store the scores, undo the move, then sort the move list by score (descending for maximizer, ascending for minimizer) before the main loop. This is essentially a &lt;strong&gt;shallow search move ordering&lt;/strong&gt;, which is known to improve pruning effectiveness (&lt;a href=\"https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:%7E:text=idea%20is%20to%20perform%20a,pruning%20of%20less%20promising%20moves\"&gt;Alpha Beta Pruning in Artificial Intelligence&lt;/a&gt;) Because our moves per position are usually limited (especially in small board games), the overhead of this sorting is small compared to the deeper search savings.&lt;/li&gt;\n&lt;li&gt;We will keep the implementation of move ordering straightforward to preserve code size. Even without complex schemes like \u201ckiller move\u201d or \u201chistory heuristic\u201d from advanced chess engines, basic ordering yields a noticeable speedup (&lt;a href=\"https://www.appliedaicourse.com/blog/alpha-beta-pruning-in-artificial-intelligence/#:%7E:text=,the%20algorithm%20even%20more%20efficient\"&gt;Alpha Beta Pruning in Artificial Intelligence&lt;/a&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Depth Limitation and Quiescence:&lt;/strong&gt; We may allow the user to specify &lt;code&gt;maxDepth&lt;/code&gt; for search. In games with potential for long forced sequences (like many jumps in checkers or multiple captures in chess), a fixed depth might cut off in the middle of a volatile situation. A full solution would use &lt;strong&gt;quiescence search&lt;/strong&gt; (continuing the search until the position is quiet, i.e., no immediate capture threats). MicroChess, for example, includes a quiescent search extension (&lt;a href=\"https://github.com/ripred/MicroChess#:%7E:text=MicroChess%20is%20an%20embedded%20chess,capture%2C%20castling%2C%20and%20quiescent%20searches\"&gt;GitHub - ripred/MicroChess: A full featured chess engine designed to fit in an embedded environment, using less than 2K of RAM!&lt;/a&gt;) However, to keep our library simpler and within time limits, we won\u2019t implement quiescence by default (it can be added for games that need it). Instead, users can slightly increase depth or incorporate capture sequences in move generation logic to mitigate the horizon effect.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory Use During Search:&lt;/strong&gt; Thanks to recursion and in-place move application, memory usage is modest. We require roughly:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Stack frame per depth:&lt;/em&gt; a few local variables (score, loop counters, etc.) plus whatever the game\u2019s &lt;code&gt;applyMove&lt;/code&gt; and &lt;code&gt;evaluateBoard&lt;/code&gt; use. Empirically, a well-optimized engine used ~142 bytes per ply in a chess scenario (&lt;a href=\"https://forum.arduino.cc/t/writing-an-embedded-chess-engine-part-5/1130748#:%7E:text=Extensive%20effort%20has%20gone%20into,running%20under%202K%20of%20RAM\"&gt;Writing an Embedded Chess Engine - Part 5 - Showcase - Arduino Forum&lt;/a&gt;) Simpler games will use far less. An 8-deep recursion for tic-tac-toe might consume well under 128 bytes total (&lt;a href=\"http://pcarduino.blogspot.com/2014/10/tic-tac-toe-on-arduino-minimax.html#:%7E:text=It%27s%20still%20possible%20to%20build,a%20lot%20more%20than%20enough\"&gt;Tic-Tac-Toe on Arduino (MiniMax)&lt;/a&gt;)&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Move list storage:&lt;/em&gt; one array of &lt;code&gt;Move&lt;/code&gt; of length MAX_MOVES, which can be on the stack or static. If static global, it uses fixed SRAM but doesn\u2019t grow with depth. If allocated on stack in each call, it multiplies by depth (which might be okay for shallow depths, but risky for deeper ones). A compromise is to use a &lt;strong&gt;global 2D buffer&lt;/strong&gt; &lt;code&gt;Move movesByDepth[MAX_DEPTH][MAX_MOVES]&lt;/code&gt; and pass a pointer to the appropriate sub-array for each recursion level. This way, we don\u2019t allocate new memory per call (it\u2019s all in global), and we avoid interference between levels. The cost is MAX_DEPTH * MAX_MOVES * sizeof(Move) bytes of SRAM. For example, if MAX_DEPTH=10 and MAX_MOVES=64 and Move=2 bytes, that\u2019s 10_64_2 = 1280 bytes, which is a large chunk of 2KB. We can tune these numbers per game or use smaller buffers if the game inherently has lower branching. Another approach is to generate moves and process them immediately, one by one, without storing the whole list \u2013 but that complicates backtracking. &lt;strong&gt;We will assume a reasonable upper bound and document that if a user\u2019s game exceeds it, they should adjust MAX_MOVES or search depth accordingly.&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Board state storage:&lt;/em&gt; If using &lt;code&gt;applyMove&lt;/code&gt;/&lt;code&gt;undoMove&lt;/code&gt;, we only maintain one copy of the board (inside the game object) plus any small info needed to undo (for instance, remembering a captured piece). This is extremely memory-efficient. If we opted to copy the board for each move instead, we\u2019d need to allocate a new board state at each node. That approach was considered in the tic-tac-toe example and quickly found impractical: even a 3-level tree consumed ~3024 bytes when copying board nodes (&lt;a href=\"http://pcarduino.blogspot.com/2014/10/tic-tac-toe-on-arduino-minimax.html#:%7E:text=Which%20is%206%20bytes%20per,children%2C%20which%20would%20give%20roughly\"&gt;Tic-Tac-Toe on Arduino (MiniMax)&lt;/a&gt;) Our in-place approach avoids that explosion. It does require that &lt;code&gt;undoMove&lt;/code&gt; correctly restores all aspects of state (board cells, whose turn, etc.), which the user must implement carefully. When done right, only a few bytes (to store what was changed) are needed per move.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;2.3 Library Integration and Usage&lt;/h1&gt;\n\n&lt;p&gt;The library will be packaged as a standard Arduino library with a header (&lt;code&gt;MinimaxAI.h&lt;/code&gt;) and source (&lt;code&gt;MinimaxAI.cpp&lt;/code&gt;), and one or more example sketches in an &lt;code&gt;examples/&lt;/code&gt; folder. It will be Arduino IDE compatible (the classes use &lt;code&gt;Arduino.h&lt;/code&gt; if needed, and avoid unsupported constructs).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Using the Library:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Include and Initialize:&lt;/strong&gt; In the user\u2019s sketch (&lt;code&gt;.ino&lt;/code&gt;), they include the library header and create an instance of their game state class (which implements the interface). They also create the Minimax solver instance, passing a reference to the game and any config (like max search depth).Depending on implementation, &lt;code&gt;MinimaxAI&lt;/code&gt; might be a class template that takes the game class type (allowing inlining and compile-time binding of game methods, which could save the overhead of virtual calls). Or it could use a base class pointer (&lt;code&gt;GameInterface *game&lt;/code&gt;) internally (simpler to understand, but each interface call is a virtual function call). Given the very low performance overhead in small games and simplicity for the user, we might go with an abstract base class &lt;code&gt;GameInterface&lt;/code&gt; that &lt;code&gt;TicTacToeGame&lt;/code&gt; inherits. For maximum speed in critical loops, a template can inline game logic, but it increases code size per game.#include &amp;lt;MinimaxAI.h&amp;gt; TicTacToeGame game;                // user-defined game state MinimaxAI&amp;lt;TicTacToeGame&amp;gt; ai(game, /*depth=*/9);  // template or concrete class instance&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Implement Game Logic:&lt;/strong&gt; The user must implement the required methods in their game class. Here\u2019s a snippet example for Tic-Tac-Toe:In the above implementation:enum Player { HUMAN = 0, AI = 1 };  // define players class TicTacToeGame : public GameInterface { public:     uint8_t board[9];    // 3x3 board stored in 1D (0 = empty, 1 = X, 2 = O)     Player current;      // whose turn it is     TicTacToeGame() {         memset(board, 0, 9);         current = AI;    // AI starts (for example)     }     int evaluateBoard() override {         // Evaluate from AI&amp;#39;s perspective (AI = &amp;#39;X&amp;#39; say = 1, Human = &amp;#39;O&amp;#39; = 2)         // Return +10 for AI win, -10 for Human win, 0 for draw/ongoing.         if (isWinner(1)) return +10;         if (isWinner(2)) return -10;         // If game not over, return 0 (or small heuristic: e.g., +1 for two-in-a-row)         return 0;     }     uint8_t generateMoves(Move *moveList) override {         uint8_t count = 0;         for (uint8_t i = 0; i &amp;lt; 9; ++i) {             if (board[i] == 0) {                 moveList[count++] = Move{i, i}; // use &amp;#39;to&amp;#39; as i; from unused             }         }         return count;     }     void applyMove(const Move &amp;amp;m) override {         uint8_t cell = &lt;a href=\"http://m.to\"&gt;m.to&lt;/a&gt;;         board[cell] = (current == AI ? 1 : 2);         // switch player turn         current = (current == AI ? HUMAN : AI);     }     void undoMove(const Move &amp;amp;m) override {         uint8_t cell = &lt;a href=\"http://m.to\"&gt;m.to&lt;/a&gt;;         // remove the mark and switch back turn         board[cell] = 0;         current = (current == AI ? HUMAN : AI);     }     bool isGameOver() override {         return isWinner(1) || isWinner(2) || isBoardFull();     }     int currentPlayer() override {         return (current == AI ? 1 : -1);          // 1 for AI (maximizing), -1 for human (minimizing)     } private:     bool isWinner(uint8_t playerVal) {         // check 3 rows, 3 cols, 2 diagonals for all == playerVal         // ... (omitted for brevity)     }     bool isBoardFull() {         for (uint8_t i = 0; i &amp;lt; 9; ++i) if (board[i] == 0) return false;         return true;     } };  We encode X as &lt;code&gt;1&lt;/code&gt; and O as &lt;code&gt;2&lt;/code&gt; on the board. The &lt;code&gt;evaluateBoard&lt;/code&gt; knows that AI is X (1) and Human is O (2), and returns positive scores for AI-winning states.&lt;code&gt;generateMoves&lt;/code&gt; lists all empty cells as possible moves.&lt;code&gt;applyMove&lt;/code&gt; and &lt;code&gt;undoMove&lt;/code&gt; simply place or remove a mark and toggle the &lt;code&gt;current&lt;/code&gt; player. (Toggling is done by checking the &lt;code&gt;current&lt;/code&gt; and swapping \u2013 since we only have two players, this is straightforward.)&lt;code&gt;currentPlayer()&lt;/code&gt; returns an int indicating if the current turn is the maximizing player or not. Here we chose AI as maximizing (return 1) and human as minimizing (return -1). The minimax solver could also determine this by comparing &lt;code&gt;current&lt;/code&gt; to a stored \u201cmaximizing player\u201d identity.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Running the AI:&lt;/strong&gt; To get the AI\u2019s move, the user calls a method from &lt;code&gt;MinimaxAI&lt;/code&gt;, for example:Internally, &lt;code&gt;findBestMove()&lt;/code&gt; will call the &lt;code&gt;minimaxSearch&lt;/code&gt; (with appropriate initial parameters: full depth, alpha=-inf, beta=+inf, and maximizing = true/false depending on &lt;code&gt;game.currentPlayer()&lt;/code&gt;). It will iterate over moves at the root to find the one that leads to the optimal score. The result is returned as a &lt;code&gt;Move&lt;/code&gt; struct. The user can then apply it to the game:(Alternatively, the &lt;code&gt;findBestMove()&lt;/code&gt; could optionally apply the move for the user, but returning it gives more flexibility.)Move best = ai.findBestMove();  game.applyMove(best);&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Serial Interface for Moves:&lt;/strong&gt; Our example sketches will demonstrate using Serial to interact:The Arduino could prompt for input like a cell number or move notation.The user enters a move (e.g., \u201c5\u201d for center cell in tic-tac-toe, or \u201c12-16\u201d in checkers notation). The sketch code will parse this and call &lt;code&gt;game.applyMove&lt;/code&gt; for the human\u2019s move.Then the AI move is computed and printed out, e.g., \u201cAI moves to 7\u201d.In a loop, this continues until &lt;code&gt;game.isGameOver()&lt;/code&gt; becomes true, at which point the result (win/draw) is announced.We ensure the example is easy to follow. For instance, we might represent tic-tac-toe board positions 1\u20139 and have the user enter a number. The code maps that to our 0\u20138 index and makes the move.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Installing and Extending:&lt;/strong&gt; The library will come with documentation (in the report and comments) describing how to define a new game class with the required methods. To use the library for another game, the user basically re-implements a class similar to &lt;code&gt;TicTacToeGame&lt;/code&gt; (for, say, &lt;code&gt;CheckersGame&lt;/code&gt; or &lt;code&gt;Connect4Game&lt;/code&gt;), writing the logic for move generation, evaluation, etc. They can then use the same &lt;code&gt;MinimaxAI&lt;/code&gt; class to get AI moves for that game. Because everything is static and no dynamic memory is used, even more complex games should fit. For example, a Checkers game might use an 8x8 board (64 bytes) and have a branching factor averaging &amp;lt; 12 moves. If we limit depth to perhaps 6 plies, the search might examine thousands of positions \u2013 which is slow but feasible (the AI may take a few seconds on Arduino for depth 6). Simpler games like Connect-4 (7x6 board) or Othello (8x8) would similarly work with adjusted depth. Chess, being much more complex, would need additional optimizations (as done in MicroChess, which uses bitboards and specialized move generation to run under 2KB (&lt;a href=\"https://github.com/ripred/MicroChess#:%7E:text=The%20design%20includes%20use%20of,games%20including%20chess%20and%20checkers\"&gt;GitHub - ripred/MicroChess: A full featured chess engine designed to fit in an embedded environment, using less than 2K of RAM!&lt;/a&gt;) , but our framework could theoretically support it at shallow depths.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory Footprint of the Library:&lt;/strong&gt; The code itself (minimax implementation) will reside in flash. We strive to keep it concise. Code size is likely on the order of a few kilobytes \u2013 well within 32KB flash. The global/static memory usage consists of the move buffer and any other static structures (which we aim to keep under a few hundred bytes). The game state itself is also often small (tic-tac-toe game state here is 9 bytes + a couple of variables; checkers might be ~32 bytes for pieces plus some bookkeeping). As a concrete data point, &lt;strong&gt;MicroChess&lt;/strong&gt; (a full chess engine) uses ~810 bytes static and leaves ~1238 bytes for runtime stack (&lt;a href=\"https://forum.arduino.cc/t/writing-an-embedded-chess-engine-part-5/1130748#:%7E:text=Extensive%20effort%20has%20gone%20into,running%20under%202K%20of%20RAM\"&gt;Writing an Embedded Chess Engine - Part 5 - Showcase - Arduino Forum&lt;/a&gt;) Our tic-tac-toe example will use far less. This shows there is ample headroom if carefully managed. By following similar strategies (bit-packing data, avoiding large arrays), one can keep within the Uno\u2019s limits for many games.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
  "likes": null,
  "suggested_sort": null,
  "banned_at_utc": null,
  "view_count": null,
  "archived": false,
  "no_follow": true,
  "is_crosspostable": false,
  "pinned": false,
  "over_18": false,
  "all_awardings": [],
  "awarders": [],
  "media_only": false,
  "link_flair_template_id": "fe7a45c6-163e-11ed-bf29-7e0b698a7bb7",
  "can_gild": false,
  "spoiler": false,
  "locked": false,
  "author_flair_text": ":snoo_facepalm:",
  "treatment_tags": [],
  "visited": false,
  "removed_by": null,
  "num_reports": null,
  "distinguished": null,
  "subreddit_id": "t5_6as6rv",
  "author_is_blocked": false,
  "mod_reason_by": null,
  "removal_reason": null,
  "link_flair_background_color": "#7193ff",
  "id": "1ihldqx",
  "is_robot_indexable": true,
  "report_reasons": null,
  "author": "ripred3",
  "discussion_type": null,
  "num_comments": 0,
  "send_replies": true,
  "contest_mode": false,
  "mod_reports": [],
  "author_patreon_flair": false,
  "author_flair_text_color": "dark",
  "permalink": "/r/ripred/comments/1ihldqx/the_amazing_minimax_algorithm_and_why_you_should/",
  "stickied": false,
  "url": "https://www.reddit.com/r/ripred/comments/1ihldqx/the_amazing_minimax_algorithm_and_why_you_should/",
  "subreddit_subscribers": 43,
  "created_utc": 1738685857.0,
  "num_crossposts": 1,
  "media": null,
  "is_video": false
}